# -*- coding: utf-8 -*-
"""FallSemesterProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K7j5nFYbENxSqVXR-276qBSF__5oFGt6
"""

!pip3 install 'torch==1.3.1'
!pip3 install 'torchvision==0.5.0'
#!pip3 install 'Pillow-SIMD'
#!pip3 install 'tqdm'

# Commented out IPython magic to ensure Python compatibility.
import os
import logging
import sys

import zipfile
import scipy.io

import time
import argparse

import math
import torch
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module

#from pygcn.layers import GraphConvolution
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import scipy.sparse as sp
import numpy as np

#from pygcn.utils import load_data, accuracy
#from pygcn.models import GCN
from sklearn.model_selection import ParameterGrid
from sklearn.model_selection import train_test_split
from torch.backends import cudnn

# %matplotlib inline
import matplotlib.pyplot as plt



# Training settings
parser = argparse.ArgumentParser()
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='Disables CUDA training.')
parser.add_argument('--fastmode', action='store_true', default=False,
                    help='Validate during training pass.')
parser.add_argument('--seed', type=int, default=42, help='Random seed.')
parser.add_argument('--epochs', type=int, default=200,
                    help='Number of epochs to train.')
parser.add_argument('--lr', type=float, default=0.01,
                    help='Initial learning rate.')
parser.add_argument('--weight_decay', type=float, default=5e-4,
                    help='Weight decay (L2 loss on parameters).')
parser.add_argument('--hidden', type=int, default=16,
                    help='Number of hidden units.')
parser.add_argument('--dropout', type=float, default=0.5,
                    help='Dropout rate (1 - keep probability).')
parser.add_argument('--earlyStop', type=int, default=10,
                    help='windows of the early stopping.')

import sys
sys.argv=['']
del sys
args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()

np.random.seed(args.seed)
torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)

if not os.path.isdir('./FallSemesterProject'):
  !git clone https://github.com/jderlich/FallSemesterProject.git

# Instantiates the layers of the model
class GraphConvolution(Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = torch.mm(input, self.weight)
        output = torch.spmm(adj, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

# The model used

class GCN(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout):
        super(GCN, self).__init__()

        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout

    def forward(self, x, adj):
        x = F.relu(self.gc1(x, adj))
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(x, adj)
        return F.log_softmax(x, dim=1)

def normalize(mx):
    """Row-normalize sparse matrix"""
    rowsum = np.array(mx.sum(1)) # make the sum on the columns 
    r_inv = np.power(rowsum, -1).flatten() # make the inverse
    r_inv[np.isinf(r_inv)] = 0. # replace the value at inf -> 0
    r_mat_inv = sp.diags(r_inv) # make the diag matrix
    mx = r_mat_inv.dot(mx) # mx is normalized
    return mx

# the Metric
def accuracy(output, labels):
    preds = output.max(1)[1].type_as(labels)
    correct = preds.eq(labels).double()
    correct = correct.sum()
    return correct / len(labels)

def sparse_mx_to_torch_sparse_tensor(sparse_mx):
    """Convert a scipy sparse matrix to a torch sparse tensor."""
    sparse_mx = sparse_mx.tocoo().astype(np.float32)
    indices = torch.from_numpy(
        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
    values = torch.from_numpy(sparse_mx.data)
    shape = torch.Size(sparse_mx.shape)
    return torch.sparse.FloatTensor(indices, values, shape)

#one hot encoder

def encode_onehot(labels):
    classes = set(labels)
    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in
                    enumerate(classes)}
    labels_onehot = np.array(list(map(classes_dict.get, labels)),
                             dtype=np.int32)
    return labels_onehot

def load_data_pubmed(path="FallSemesterProject/data/", dataset="pubmed", flag = False):
    """Load citation network dataset (cora only for now)"""
    path = path+dataset+'/'
    print('Loading {} dataset...'.format(dataset))

    idx_labels = np.genfromtxt("{}{}.label".format(path, dataset),
                                        dtype=np.dtype(str))

    labels = encode_onehot(idx_labels[:, -1])

    #unzip the file containing the features if no yet done
    if (flag == True):
      with zipfile.ZipFile("{}{}.feature.zip".format(path, dataset), 'r') as zip_ref:
        zip_ref.extractall("./dataset_unzipped/")
    #load the features
    mat = scipy.io.loadmat("./dataset_unzipped/pubmed.feature.mat")
    features = mat['feature']
    features[features != 0] = 1
    features = sp.csr_matrix(features, dtype=np.float32)

    # build graph
    idx = np.array(idx_labels[:, 0], dtype=np.int64)
    idx_map = {j: i for i, j in enumerate(idx)}

    edges_unordered = np.genfromtxt("{}{}.edgelist".format(path, dataset),
                                    dtype=np.int32)
      
    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),
                     dtype=np.int32).reshape(edges_unordered.shape)
    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),
                        shape=(labels.shape[0], labels.shape[0]),
                        dtype=np.float32)

    # build symmetric adjacency matrix
    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

    features = normalize(features)
    adj = normalize(adj + sp.eye(adj.shape[0]))

    features = torch.FloatTensor(np.array(features.todense()))
    labels = torch.LongTensor(np.where(labels)[1])
    adj = sparse_mx_to_torch_sparse_tensor(adj)

    return adj, features, labels

# load the data
def load_data(path="FallSemesterProject/data/", dataset="cora"):
    """Load citation network dataset (cora only for now)"""
    path = path+dataset+'/'
    print('Loading {} dataset...'.format(dataset))

    idx_features_labels = np.genfromtxt("{}{}.content".format(path, dataset),
                                        dtype=np.dtype(str))
    if dataset =='citeseer':
      idx_features_labels, mappa = map_docID_to_int(idx_features_labels)

    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)
    labels = encode_onehot(idx_features_labels[:, -1])

    # build graph
    idx = np.array(idx_features_labels[:, 0], dtype=np.int64)
    idx_map = {j: i for i, j in enumerate(idx)}
    if dataset == 'citeseer':
      edges_unordered = np.genfromtxt("{}{}.cites".format(path, dataset),
                                    dtype=np.dtype(str))
      edges_unordered= map_edges(mappa,edges_unordered)
    else:
      edges_unordered = np.genfromtxt("{}{}.cites".format(path, dataset),
                                    dtype=np.int32)
      
    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),
                     dtype=np.int32).reshape(edges_unordered.shape)
    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),
                        shape=(labels.shape[0], labels.shape[0]),
                        dtype=np.float32)

    # build symmetric adjacency matrix
    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

    features = normalize(features)
    adj = normalize(adj + sp.eye(adj.shape[0]))


    features = torch.FloatTensor(np.array(features.todense()))
    labels = torch.LongTensor(np.where(labels)[1])
    adj = sparse_mx_to_torch_sparse_tensor(adj)
    return adj, features, labels

"""####*Data cleaning on Citeseer dataset*"""

# map each doc_name with a unique id
# return the mapped feature array and the map itself
def map_docID_to_int(features):
  mappa = dict()
  counter = 0;
  for num in range(len(features)):
    idx = features[num,0]
    mappa[idx] = counter
    features[num,0] = counter
    counter += 1
  return features, mappa

# map everynode in each edge in the edges array
def map_edges(mappa,edges ):
  res = []
  lung = len(edges)
  for id in range(lung):
    tmp2 = []

    tmp = mappa.get(str(edges[id,0]))
    if tmp is None:
      continue
    tmp2.append(tmp)

    tmp = mappa.get(str(edges[id,1]))
    if tmp is None:
      continue
    tmp2.append(tmp)

    res.append(tmp2)

  return np.array(res, dtype= np.int32)

"""####*Split the dataset into train, validation, and test sets*"""

def find_indexes(features, d = 1):

  idx = range(features.shape[0])
  idx_train, idx_test, y_train, _ = train_test_split(
    idx, idx, test_size=1000, random_state=1)
  idx_train, idx_val, y_train, _ = train_test_split(
    idx_train, y_train, test_size=500, random_state=1)
  d = 1
  if d != 1:
    num = int((features.shape[0] * d) // 1) +1
    idx_train, _, _, _ = train_test_split(
      idx_train, y_train, train_size = num, random_state=1)
  
  idx_train = torch.LongTensor(idx_train)
  idx_val = torch.LongTensor(idx_val)
  idx_test = torch.LongTensor(idx_test)

  return idx_train, idx_val, idx_test

"""###*Create an instance of the model*"""

def make_model(args, features, labels):

  if args.cuda:
    model.cuda()
    features = features.cuda()
    adj = adj.cuda()
    labels = labels.cuda()
    idx_train = idx_train.cuda()
    idx_val = idx_val.cuda()
    idx_test = idx_test.cuda()
  # Model and optimizer
  model = GCN(nfeat=features.shape[1],
              nhid=args.hidden,
              nclass=labels.max().item() + 1,
              dropout=args.dropout)
  optimizer = optim.Adam(model.parameters(),
                       lr=args.lr, weight_decay=args.weight_decay)

  return model, optimizer

"""####*Plot function*"""

def plot_func(epochs, accuracies, name, title):
  fig = plt.gcf()
  fig.set_size_inches(12.5, 7.5)
  x = range(epochs+1)
  # print(len(x))
  # print(len(accuracies))
  fig = plt.figure
  plt.plot(x, accuracies)

  plt.ylabel('score', fontsize = 16)
  plt.xlabel('# Epoch', fontsize = 16)

  plt.title(title, fontsize = 20)

  plt.grid()

  plt.savefig(name, dpi=100)
  plt.show()

# Train the given model
def train(epoch, model, optimizer, adj, labels, idx_train, idx_val, features):
    t = time.time()
    model.train()
    optimizer.zero_grad()
    output = model(features, adj)
    loss_train = F.nll_loss(output[idx_train], labels[idx_train])
    acc_train = accuracy(output[idx_train], labels[idx_train])
    loss_train.backward()
    optimizer.step()

    if not args.fastmode:
        # Evaluate validation set performance separately,
        # deactivates dropout during validation run.
        model.eval()
        output = model(features, adj)

    loss_val = F.nll_loss(output[idx_val], labels[idx_val])
    acc_val = accuracy(output[idx_val], labels[idx_val])
    print('Epoch: {:04d}'.format(epoch+1),
          'loss_train: {:.4f}'.format(loss_train.item()),
          'acc_train: {:.4f}'.format(acc_train.item()),
          'loss_val: {:.4f}'.format(loss_val.item()),
          'acc_val: {:.4f}'.format(acc_val.item()),
          'time: {:.4f}s'.format(time.time() - t))
    return loss_val.item(), acc_train.item(), acc_val.item()

# test the given model
def test(model,features, adj, labels, idx_test):
    model.eval()
    output = model(features, adj)
    loss_test = F.nll_loss(output[idx_test], labels[idx_test])
    acc_test = accuracy(output[idx_test], labels[idx_test])
    print("Test set results:",
          "loss= {:.4f}".format(loss_test.item()),
          "accuracy= {:.4f}".format(acc_test.item()))
    return acc_test.item()

"""####*Perform the Hyper parameter Tuning using different models*"""

def train_iterator(args, features, labels, adj, idx_train, idx_val):
  #  early stopping counter
  val_acc_list = []
  stop_counter = 0;
  val_loss = 1;
  train_acc = 0;
  eff_epoch = 0;

  # instantiate the model
  model, optimizer = make_model(args, features, labels)

  # Train model
  t_total = time.time()
  for epoch in range(args.epochs):
    tmp_val, train_acc, val_acc = train(epoch, model, optimizer, adj, labels, idx_train, idx_val, features)
    val_acc_list.append(val_acc)
    eff_epoch = epoch
    if ( val_loss == tmp_val):
        stop_counter += 1
    else:
        val_loss = tmp_val
        stop_counter = 0
  
    if (stop_counter == args.earlyStop):
        print('EARLY STOPPING at Epoch {}'.format(epoch))
        eff_epoch = epoch
        break
  
  print("Optimization Finished!")
  print("Total time elapsed: {:.4f}s".format(time.time() - t_total))
  return val_acc, model, val_acc_list, eff_epoch;

"""####*Multiple run Using different random selection for (train validation, and test sets)*"""

def multiple_run_with_rand_split(args, features, labels, adj, idx_train, idx_val, idx_test, d = 1):
  # load random indexes
  # args already contained the best parameters found previously
  idx_train, idx_val, idx_test = find_indexes(features, d)
  _, model, _, _ = train_iterator(args, features, labels, adj, idx_train, idx_val)
  # Testing the best model
  acc_test = test(model,features, adj, labels, idx_test )
  return acc_test

def multiple_run_scores(args, features, labels, adj, idx_train, idx_val, idx_test, d = 1):
  acc_test_list = []
  for i in range(10):
    acc_test = multiple_run_with_rand_split(args, features, labels, adj, idx_train, idx_val, idx_test, d)
    acc_test_list.append(acc_test)
  acc_test_array = np.array(acc_test_list)
  print('Cora dataset')
  print('accuracy confidence interval with mean accuracy = {:.2f}% and std = {:.4f}'.format(np.mean(acc_test_array)*100,np.std(acc_test_array) ))
  return acc_test_list

"""###*1) CORA Data Set*"""

# Load data
adj, features, labels = load_data(dataset= 'cora')
# load the indexes
idx_train, idx_val, idx_test = find_indexes(features, d = 0.052)
# the dataset is not balanced regarding the label of the nodes

"""###*Hyper parameter tuning (cora)*"""

# validation accuracies
accuracies = [] 
param_grid = {
    'lr' : [1e-2, 5e-2, 8e-3],
    'weight_decay': [5e-4, 5e-3, 9e-4],
    'epoch': [200, 250, 300]
}
for params in ParameterGrid(param_grid):
  args.lr = params['lr']
  args.weight_decay = params['weight_decay']
  args.epochs = params['epoch']
  print('learning rate = {}, weight decay = {}, epochs = {}'.format(args.lr, args.weight_decay, args.epochs))
  tmp_acc, _, _, _ = train_iterator(args, features, labels, adj, idx_train, idx_val)
  accuracies.append(tmp_acc)

id = np.argmax(accuracies)
best_params = ParameterGrid(param_grid)[id]
print(best_params)
args.lr = best_params['lr']
args.weight_decay = best_params['weight_decay']
args.epochs = best_params['epoch']
_, best_model, val_acc_list, eff_epoch = train_iterator(args, features, labels, adj, idx_train, idx_val)

plot_func(eff_epoch, val_acc_list, 'cora_valAcc_trend', 'Model Learning Curve(Cora)')

# Testing the best model
test(best_model,features, adj, labels, idx_test)

"""###*Rand split*

"""

list_acc = multiple_run_scores(args, features, labels, adj, idx_train, idx_val, idx_test, d= 0.052)

"""###*2) CITESEER Data Set*"""

# Load data
adj2, features2, labels2 = load_data(dataset= 'citeseer')
# load the indexes
idx_train2, idx_val2, idx_test2 = find_indexes(features, d = 0.036)
# the dataset is not balanced regarding the label of the nodes

"""###*Hyper parameter tuning (citeseer)*"""

# validation accuracies
accuracies2 = [] 
param_grid = {
    'lr' : [1e-2, 5e-2, 8e-3],
    'weight_decay': [5e-4, 5e-3, 9e-4],
    'epoch': [200, 250, 300]
}
for params in ParameterGrid(param_grid):
  args.lr = params['lr']
  args.weight_decay = params['weight_decay']
  args.epochs = params['epoch']
  print('learning rate = {}, weight decay = {}, epochs = {}'.format(args.lr, args.weight_decay, args.epochs))
  tmp_acc, _, _, _ = train_iterator(args, features2, labels2, adj2, idx_train2, idx_val2)
  accuracies2.append(tmp_acc)

id2 = np.argmax(accuracies2)
best_params2 = ParameterGrid(param_grid)[id2]
print(best_params2)
args.lr = best_params2['lr']
args.weight_decay = best_params2['weight_decay']
args.epochs = best_params2['epoch']
_, best_model2, val_acc_list2, eff_epoch2 = train_iterator(args, features2, labels2, adj2, idx_train2, idx_val2)

plot_func(eff_epoch2, val_acc_list2, 'citeseer_valAcc_trend', 'Model Learning Curve(Citeseer)')

# Testing the best model
test(best_model2,features2, adj2, labels2, idx_test2)

list_acc2 = multiple_run_scores(args, features2, labels2, adj2, idx_train2, idx_val2, idx_test2, d=0.036)

"""###*3) PUBMED DataSet*"""

# Load data
adj3, features3, labels3 = load_data_pubmed(flag= True)
# load the indexes
idx_train3, idx_val3, idx_test3 = find_indexes(features3, d = 0.003)
# the dataset is not balanced regarding the label of the nodes

"""####*Hyper parameter Tuning(Pubmed)*"""

# validation accuracies
accuracies3 = [] 
param_grid = {
    'lr' : [1e-2, 5e-2, 8e-3],
    'weight_decay': [5e-4, 5e-3, 9e-4],
    'epoch': [200, 250, 300]
}
for params in ParameterGrid(param_grid):
  args.lr = params['lr']
  args.weight_decay = params['weight_decay']
  args.epochs = params['epoch']
  print('learning rate = {}, weight decay = {}, epochs = {}'.format(args.lr, args.weight_decay, args.epochs))
  tmp_acc, _, _, _ = train_iterator(args, features3, labels3, adj3, idx_train3, idx_val3)
  accuracies3.append(tmp_acc)

id3 = np.argmax(accuracies3)
best_params3 = ParameterGrid(param_grid)[id3]
print(best_params3)
args.lr = best_params3['lr']
args.weight_decay = best_params3['weight_decay']
args.epochs = best_params3['epoch']
_, best_model3, val_acc_list3, eff_epoch3 = train_iterator(args, features3, labels3, adj3, idx_train3, idx_val3)

plot_func(eff_epoch3, val_acc_list3, 'pubmed_valAcc_trend', 'Model Learning Curve(Pubmed)')

# Testing the best model
test(best_model3,features3, adj3, labels3, idx_test3)

list_acc3 = multiple_run_scores(args, features3, labels3, adj3, idx_train3, idx_val3, idx_test3, d = 0.003)

"""####*Plot in the same Figure*"""

def plot_3(epoch_c, cora, epoch_ci, citeseer, epoch_p, pubmed):
  x = range(epoch_c)
  fig = plt.gcf()
  fig.set_size_inches(10.5, 7.5)  
  fig = plt.figure
  plt.plot(x, cora, label='cora_valAcc_trend')
  x = range(epoch_ci)
  plt.plot(x, citeseer, label='citeseer_valAcc_trend')
  x = range(epoch_p)
  plt.plot(x, pubmed, label='pubmed_valAcc_trend')

  plt.ylabel('Score', fontsize = 16)
  plt.xlabel('# Epochs', fontsize = 16)

  plt.title('Models Learning curves', fontsize = 20)

  plt.legend(fontsize = 16)
  plt.grid()

  plt.savefig('summary.png', dpi=100)
  plt.show()

plot_3(best_params['epoch'], val_acc_list, best_params2['epoch'], val_acc_list2, best_params3['epoch'], val_acc_list3 )

"""##*Planetoide*"""

if not os.path.isdir('./GraphGCN'):
  !git clone https://github.com/jderlich/GraphGCN.git

!pip install theano==0.7

!rm -rf ~/.theano
#!sudo pip install  pydot==1.4.1

!sudo pip install graphviz
!sudo apt-get install graphviz

!pip install theano --upgrade

!sudo pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

import theano
import lasagne
import pickle as cPickle

class base_model(object):
    """the base model for both transductive and inductive learning."""

    def __init__(self, args):
        """
        args (an object): contains the arguments used for initalizing the model.
        """
        self.embedding_size = args.embedding_size
        self.learning_rate = args.learning_rate
        self.batch_size = args.batch_size
        self.neg_samp = args.neg_samp
        self.model_file = args.model_file
        
        self.window_size = args.window_size
        self.path_size = args.path_size
        
        self.g_batch_size = args.g_batch_size
        self.g_learning_rate = args.g_learning_rate
        self.g_sample_size = args.g_sample_size

        self.use_feature = args.use_feature
        self.update_emb = args.update_emb
        self.layer_loss = args.layer_loss

        lasagne.random.set_rng(np.random)
        np.random.seed(13)

        random.seed(13)

        self.inst_generator = self.gen_train_inst()
        self.graph_generator = self.gen_graph()
        self.label_generator = self.gen_label_graph()

    def store_params(self):
        """serialize the model parameters in self.model_file.
        """

        for i, l in enumerate(self.l):
            fout = open("{}.{}".format(self.model_file, i), 'w')
            params = lasagne.layers.get_all_param_values(l)
            cPickle.dump(params, fout, cPickle.HIGHEST_PROTOCOL)
            fout.close()

    def load_params(self):
        """load the model parameters from self.model_file.
        """
        for i, l in enumerate(self.l):
            fin = open("{}.{}".format(self.model_file, i))
            params = cPickle.load(fin)
            lasagne.layers.set_all_param_values(l, params)
            fin.close()

    def comp_iter(self, iter):
        """an auxiliary function used for computing the number of iterations given the argument iter.
        iter can either be an int or a float.
        """
        if iter >= 1:
            return iter
        return 1 if random.random() < iter else 0

    def train(self, init_iter_label, init_iter_graph, max_iter, iter_graph, iter_inst, iter_label):
        """training API.
        This method is a wrapper for init_train and step_train.
        Refer to init_train and step_train for more details (Cf. trans_model.py and ind_model.py).
        """
        self.init_train(init_iter_label, init_iter_graph)
        self.step_train(max_iter, iter_graph, iter_inst, iter_label)

from theano import sparse
import theano.tensor as T
import lasagne
import GraphGCN.layers
import theano
import numpy as np
import random
from collections import defaultdict as dd

from GraphGCN.base_model import base_model

class ind_model(base_model):
    """Planetoid-I.
    """

    def add_data(self, x, y, allx, graph):
        """add data to the model.
        x (scipy.sparse.csr_matrix): feature vectors for labeled training data.
        y (numpy.ndarray): one-hot label encoding for labeled training data.
        allx (scipy.sparse.csr_matrix): feature vectors for both labeled and unlabeled data.
        graph (dict): the format is {index: list_of_neighbor_index}. Only supports binary graph.
        Let n be the number of training (both labeled and unlabeled) training instances.
        These n instances should be indexed from 1 to n - 1 in the graph with the same order in allx.
        """
        self.x, self.y, self.allx, self.graph = x, y, allx, graph
        self.num_ver = self.allx.shape[0]

    def build(self):
        """build the model. This method should be called after self.add_data.
        """
        x_sym = sparse.csr_matrix('x', dtype = 'float32')
        self.x_sym = x_sym
        y_sym = T.imatrix('y')
        gx_sym = sparse.csr_matrix('gx', dtype = 'float32')
        gy_sym = T.ivector('gy')
        gz_sym = T.vector('gz')

        l_x_in = lasagne.layers.InputLayer(shape = (None, self.x.shape[1]), input_var = x_sym)
        l_gx_in = lasagne.layers.InputLayer(shape = (None, self.x.shape[1]), input_var = gx_sym)
        l_gy_in = lasagne.layers.InputLayer(shape = (None, ), input_var = gy_sym)

        l_x_1 = layers.SparseLayer(l_x_in, self.y.shape[1], nonlinearity = lasagne.nonlinearities.softmax)
        l_x_2 = layers.SparseLayer(l_x_in, self.embedding_size)
        W = l_x_2.W
        l_x_2 = layers.DenseLayer(l_x_2, self.y.shape[1], nonlinearity = lasagne.nonlinearities.softmax)
        if self.use_feature:
            l_x = lasagne.layers.ConcatLayer([l_x_1, l_x_2], axis = 1)
            l_x = layers.DenseLayer(l_x, self.y.shape[1], nonlinearity = lasagne.nonlinearities.softmax)
        else:
            l_x = l_x_2

        l_gx = layers.SparseLayer(l_gx_in, self.embedding_size, W = W)
        if self.neg_samp > 0:
            l_gy = lasagne.layers.EmbeddingLayer(l_gy_in, input_size = self.num_ver, output_size = self.embedding_size)
            l_gx = lasagne.layers.ElemwiseMergeLayer([l_gx, l_gy], T.mul)
            pgy_sym = lasagne.layers.get_output(l_gx)
            g_loss = - T.log(T.nnet.sigmoid(T.sum(pgy_sym, axis = 1) * gz_sym)).sum()
        else:
            l_gx = lasagne.layers.DenseLayer(l_gx, self.num_ver, nonlinearity = lasagne.nonlinearities.softmax)
            pgy_sym = lasagne.layers.get_output(l_gx)
            g_loss = lasagne.objectives.categorical_crossentropy(pgy_sym, gy_sym).sum()
        
        self.l = [l_x, l_gx]

        py_sym = lasagne.layers.get_output(l_x)
        loss = lasagne.objectives.categorical_crossentropy(py_sym, y_sym).mean()
        if self.layer_loss and self.use_feature:
            hid_sym = lasagne.layers.get_output(l_x_1)
            loss += lasagne.objectives.categorical_crossentropy(hid_sym, y_sym).mean()
            emd_sym = lasagne.layers.get_output(l_x_2)
            loss += lasagne.objectives.categorical_crossentropy(emd_sym, y_sym).mean()

        params = [l_x_1.W, l_x_1.b, l_x_2.W, l_x_2.b, l_x.W, l_x.b] if self.use_feature else [l_x.W, l_x.b]
        if self.update_emb:
            params = lasagne.layers.get_all_params(l_x)
        updates = lasagne.updates.sgd(loss, params, learning_rate = self.learning_rate)
        self.train_fn = theano.function([x_sym, y_sym], loss, updates = updates)

        g_params = lasagne.layers.get_all_params(l_gx)
        g_updates = lasagne.updates.sgd(g_loss, g_params, learning_rate = self.g_learning_rate)
        self.g_fn = theano.function([gx_sym, gy_sym, gz_sym], g_loss, updates = g_updates, on_unused_input = 'ignore')

        self.test_fn = theano.function([x_sym], py_sym)

    def gen_train_inst(self):
        """generator for batches for classification loss.
        """
        while True:
            ind = np.array(np.random.permutation(self.x.shape[0]), dtype = np.int32)
            i = 0
            while i < self.x.shape[0]:
                j = min(ind.shape[0], i + self.batch_size)
                yield self.x[ind[i: j]], self.y[ind[i: j]]
                i = j

    def gen_graph(self):
        """generator for batches for graph context loss.
        """
        while True:
            ind = np.random.permutation(self.num_ver)
            i = 0
            while i < ind.shape[0]:
                g, gy = [], []
                j = min(ind.shape[0], i + self.g_batch_size)
                for k in ind[i: j]:
                    if len(self.graph[k]) == 0: continue
                    path = [k]
                    for _ in range(self.path_size):
                        path.append(random.choice(self.graph[path[-1]]))
                    for l in range(len(path)):
                        if path[l] >= self.allx.shape[0]: continue
                        for m in range(l - self.window_size, l + self.window_size + 1):
                            if m < 0 or m >= len(path): continue
                            if path[m] >= self.allx.shape[0]: continue
                            g.append([path[l], path[m]])
                            gy.append(1.0)
                            for _ in range(self.neg_samp):
                                g.append([path[l], random.randint(0, self.num_ver - 1)])
                                gy.append(- 1.0)
                g = np.array(g, dtype = np.int32)
                yield self.allx[g[:, 0]], g[:, 1], gy
                i = j

    def gen_label_graph(self):
        """generator for batches for label context loss.
        """
        labels, label2inst, not_label = [], dd(list), dd(list)
        for i in range(self.x.shape[0]):
            flag = False
            for j in range(self.y.shape[1]):
                if self.y[i, j] == 1 and not flag:
                    labels.append(j)
                    label2inst[j].append(i)
                    flag = True
                elif self.y[i, j] == 0:
                    not_label[j].append(i)

        while True:
            g, gy = [], []
            for _ in range(self.g_sample_size):
                x1 = random.randint(0, self.x.shape[0] - 1)
                label = labels[x1]
                if len(label2inst) == 1: continue
                x2 = random.choice(label2inst[label])
                g.append([x1, x2])
                gy.append(1.0)
                for _ in range(self.neg_samp):
                    g.append([x1, random.choice(not_label[label])])
                    gy.append(- 1.0)
            g = np.array(g, dtype = np.int32)
            yield self.allx[g[:, 0]], g[:, 1], gy


    def init_train(self, init_iter_label, init_iter_graph):
        """pre-training of graph embeddings.
        init_iter_label (int): # iterations for optimizing label context loss.
        init_iter_graph (int): # iterations for optimizing graph context loss.
        """
        for i in range(init_iter_label):
            gx, gy, gz = next(self.label_generator)
            loss = self.g_fn(gx, gy, gz)
            print ('iter label', i, loss)

        for i in range(init_iter_graph):
            gx, gy, gz = next(self.graph_generator)
            loss = self.g_fn(gx, gy, gz)
            print ('iter graph', i, loss)

    def step_train(self, max_iter, iter_graph, iter_inst, iter_label):
        """a training step. Iteratively sample batches for three loss functions.
        max_iter (int): # iterations for the current training step.
        iter_graph (int): # iterations for optimizing the graph context loss.
        iter_inst (int): # iterations for optimizing the classification loss.
        iter_label (int): # iterations for optimizing the label context loss.
        """
        for _ in range(max_iter):
            for _ in range(self.comp_iter(iter_graph)):
                gx, gy, gz = next(self.graph_generator)
                self.g_fn(gx, gy, gz)
            for _ in range(self.comp_iter(iter_inst)):
                x, y = next(self.inst_generator)
                self.train_fn(x, y)
            for _ in range(self.comp_iter(iter_label)):
                gx, gy, gz = next(self.label_generator)
                self.g_fn(gx, gy, gz)

    def predict(self, tx):
        """predict the dev or test instances.
        tx (scipy.sparse.csr_matrix): feature vectors for dev instances.
        returns (numpy.ndarray, #instacnes * #classes): classification probabilities for dev instances.
        """
        return self.test_fn(tx)

#from GraphGCN.ind_model import ind_model as model
import argparse

DATASET = 'citeseer'

parser = argparse.ArgumentParser()
parser.add_argument('--learning_rate', help = 'learning rate for supervised loss', type = float, default = 0.1)
parser.add_argument('--embedding_size', help = 'embedding dimensions', type = int, default = 50)
parser.add_argument('--window_size', help = 'window size in random walk sequences', type = int, default = 3)
parser.add_argument('--path_size', help = 'length of random walk sequences', type = int, default = 10)
parser.add_argument('--batch_size', help = 'batch size for supervised loss', type = int, default = 200)
parser.add_argument('--g_batch_size', help = 'batch size for graph context loss', type = int, default = 20)
parser.add_argument('--g_sample_size', help = 'batch size for label context loss', type = int, default = 20)
parser.add_argument('--neg_samp', help = 'negative sampling rate; zero means using softmax', type = int, default = 0)
parser.add_argument('--g_learning_rate', help = 'learning rate for unsupervised loss', type = float, default = 1e-3)
parser.add_argument('--model_file', help = 'filename for saving models', type = str, default = 'ind.model')
parser.add_argument('--use_feature', help = 'whether use input features', type = bool, default = True)
parser.add_argument('--update_emb', help = 'whether update embedding when optimizing supervised loss', type = bool, default = True)
parser.add_argument('--layer_loss', help = 'whether incur loss on hidden layers', type = bool, default = True)
args = parser.parse_args()

def comp_accu(tpy, ty):
    import numpy as np
    return (np.argmax(tpy, axis = 1) == np.argmax(ty, axis = 1)).sum() * 1.0 / tpy.shape[0]

# load the data: x, y, tx, ty, allx, graph
NAMES = ['x', 'y', 'tx', 'ty', 'allx', 'graph']
OBJECTS = []
for i in range(len(NAMES)):
    OBJECTS.append(cPickle.load(open("GraphGCN/data/ind.{}.{}".format(DATASET, NAMES[i]))))
x, y, tx, ty, allx, graph = tuple(OBJECTS)

m = ind_model(args)                                                 # initialize the model
m.add_data(x, y, allx, graph)                                   # add data
m.build()                                                       # build the model
m.init_train(init_iter_label = 10000, init_iter_graph = 400)    # pre-training
iter_cnt, max_accu = 0, 0
while True:
    m.step_train(max_iter = 1, iter_graph = 0.1, iter_inst = 1, iter_label = 0) # perform a training step
    tpy = m.predict(tx)                                                         # predict the dev set
    accu = comp_accu(tpy, ty)                                                   # compute the accuracy on the dev set
    print (iter_cnt, accu, max_accu)
    iter_cnt += 1
    if accu > max_accu:
        m.store_params()                                                        # store the model if better result is obtained
        max_accu = max(max_accu, accu)

